{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:17.064143Z",
     "iopub.status.busy": "2025-08-14T10:58:17.063315Z",
     "iopub.status.idle": "2025-08-14T10:58:17.067741Z",
     "shell.execute_reply": "2025-08-14T10:58:17.066846Z",
     "shell.execute_reply.started": "2025-08-14T10:58:17.064117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:17.069134Z",
     "iopub.status.busy": "2025-08-14T10:58:17.068947Z",
     "iopub.status.idle": "2025-08-14T10:58:17.086310Z",
     "shell.execute_reply": "2025-08-14T10:58:17.085415Z",
     "shell.execute_reply.started": "2025-08-14T10:58:17.069120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import List, Literal, Union\n",
    "\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from lightning.pytorch import callbacks as lcb\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim, utils\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryAUROC\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaModel,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaPreTrainedModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:17.087425Z",
     "iopub.status.busy": "2025-08-14T10:58:17.087104Z",
     "iopub.status.idle": "2025-08-14T10:58:17.103739Z",
     "shell.execute_reply": "2025-08-14T10:58:17.102938Z",
     "shell.execute_reply.started": "2025-08-14T10:58:17.087402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:17.184088Z",
     "iopub.status.busy": "2025-08-14T10:58:17.183862Z",
     "iopub.status.idle": "2025-08-14T10:58:17.213800Z",
     "shell.execute_reply": "2025-08-14T10:58:17.213090Z",
     "shell.execute_reply.started": "2025-08-14T10:58:17.184072Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import inspect\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "\n",
    "def clean_mem():\n",
    "    # import gc\n",
    "    # import os\n",
    "    # import sys\n",
    "    # import time\n",
    "    # import traceback\n",
    "\n",
    "    # import psutil\n",
    "    # import torch\n",
    "\n",
    "    process = psutil.Process(os.getpid())\n",
    "\n",
    "    # Measure RAM before cleanup\n",
    "    ram_before = process.memory_info().rss / (1024**2)  # in MB\n",
    "\n",
    "    # Measure GPU before cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_alloc_before = torch.cuda.memory_allocated() / (1024**2)  # in MB\n",
    "        gpu_reserved_before = torch.cuda.memory_reserved() / (1024**2)  # in MB\n",
    "    else:\n",
    "        gpu_alloc_before = gpu_reserved_before = 0\n",
    "\n",
    "    # clean all traceback\n",
    "    if hasattr(sys, \"last_traceback\"):\n",
    "        traceback.clear_frames(sys.last_traceback)\n",
    "        delattr(sys, \"last_traceback\")\n",
    "    if hasattr(sys, \"last_type\"):\n",
    "        delattr(sys, \"last_type\")\n",
    "    if hasattr(sys, \"last_value\"):\n",
    "        delattr(sys, \"last_value\")\n",
    "\n",
    "    # clean all ipython history\n",
    "    if \"get_ipython\" in globals():\n",
    "        try:\n",
    "            from IPython import get_ipython\n",
    "\n",
    "            ip = get_ipython()\n",
    "            user_ns = ip.user_ns\n",
    "            ip.displayhook.flush()\n",
    "            pc = ip.displayhook.prompt_count + 1\n",
    "            for n in range(1, pc):\n",
    "                user_ns.pop(\"_i\" + repr(n), None)\n",
    "            user_ns.update(dict(_i=\"\", _ii=\"\", _iii=\"\"))\n",
    "            hm = ip.history_manager\n",
    "            hm.input_hist_parsed[:] = [\"\"] * pc\n",
    "            hm.input_hist_raw[:] = [\"\"] * pc\n",
    "            hm._i = hm._ii = hm._iii = hm._i00 = \"\"\n",
    "        except Exception as e:\n",
    "            print(\"ipython mem could not be cleared\")\n",
    "\n",
    "    # do a garbage collection and flush cuda cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Give system a small moment to settle (helps RAM measurement be more accurate)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    # Measure RAM after cleanup\n",
    "    ram_after = process.memory_info().rss / (1024**2)  # in MB\n",
    "\n",
    "    # Measure GPU after cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_alloc_after = torch.cuda.memory_allocated() / (1024**2)  # in MB\n",
    "        gpu_reserved_after = torch.cuda.memory_reserved() / (1024**2)  # in MB\n",
    "    else:\n",
    "        gpu_alloc_after = gpu_reserved_after = 0\n",
    "\n",
    "    # Report freed memory\n",
    "    print(\n",
    "        f\"RAM freed: {ram_before - ram_after:.2f} MB ({ram_before:.2f} -> {ram_after:.2f})\"\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        print(\n",
    "            f\"GPU allocated freed: {gpu_alloc_before - gpu_alloc_after:.2f} MB ({gpu_alloc_before:.2f} -> {gpu_alloc_after:.2f})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"GPU reserved freed: {gpu_reserved_before - gpu_reserved_after:.2f} MB ({gpu_reserved_before:.2f} -> {gpu_reserved_after:.2f})\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"No GPU detected.\")\n",
    "\n",
    "\n",
    "def create_logger(\n",
    "    name: str = \"reddit_moderation\",\n",
    "    log_level: str = \"INFO\",\n",
    "    log_file: Optional[Union[str, Path]] = None,\n",
    "    log_dir: Optional[Union[str, Path]] = \"logs\",\n",
    "    console_output: bool = True,\n",
    "    file_output: bool = True,\n",
    "    format_string: Optional[str] = None,\n",
    "    max_bytes: int = 10_000_000,  # 10MB\n",
    "    backup_count: int = 5,\n",
    "    include_timestamp_in_filename: bool = True,\n",
    ") -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Create a fully featured logger for the Reddit comment moderation system.\n",
    "\n",
    "    This logger is designed to handle all aspects of the multi-stage classification\n",
    "    pipeline including zero-shot classification, fine-tuning, and evaluation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str, optional\n",
    "        Logger name, by default \"reddit_moderation\"\n",
    "    log_level : str, optional\n",
    "        Logging level (\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"),\n",
    "        by default \"INFO\"\n",
    "    log_file : str or Path, optional\n",
    "        Specific log file path. If None, auto-generates based on name and timestamp\n",
    "    log_dir : str or Path, optional\n",
    "        Directory for log files, by default \"logs\"\n",
    "    console_output : bool, optional\n",
    "        Whether to output logs to console, by default True\n",
    "    file_output : bool, optional\n",
    "        Whether to output logs to file, by default True\n",
    "    format_string : str, optional\n",
    "        Custom log format string, by default None (uses comprehensive format)\n",
    "    max_bytes : int, optional\n",
    "        Maximum log file size before rotation, by default 10MB\n",
    "    backup_count : int, optional\n",
    "        Number of backup log files to keep, by default 5\n",
    "    include_timestamp_in_filename : bool, optional\n",
    "        Whether to include timestamp in log filename, by default True\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    logging.Logger\n",
    "        Configured logger instance ready for use\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Basic usage\n",
    "    >>> logger = create_logger()\n",
    "    >>> logger.info(\"Starting Reddit comment classification pipeline\")\n",
    "\n",
    "    >>> # Advanced usage for training\n",
    "    >>> training_logger = create_logger(\n",
    "    ...     name=\"distilbert_training\",\n",
    "    ...     log_level=\"DEBUG\",\n",
    "    ...     log_file=\"training_session.log\"\n",
    "    ... )\n",
    "    >>> training_logger.debug(\"Training batch processed\")\n",
    "\n",
    "    >>> # For evaluation only\n",
    "    >>> eval_logger = create_logger(\n",
    "    ...     name=\"model_evaluation\",\n",
    "    ...     console_output=False,\n",
    "    ...     log_file=\"evaluation_results.log\"\n",
    "    ... )\n",
    "    \"\"\"\n",
    "\n",
    "    # Create logger\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(getattr(logging, log_level.upper()))\n",
    "\n",
    "    # Clear existing handlers to avoid duplication\n",
    "    logger.handlers.clear()\n",
    "\n",
    "    # Default comprehensive format for ML workflows\n",
    "    if format_string is None:\n",
    "        format_string = \"%(asctime)s | %(name)s | %(levelname)s | %(message)s\"\n",
    "\n",
    "    formatter = logging.Formatter(format_string, datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Console handler\n",
    "    if console_output:\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setLevel(getattr(logging, log_level.upper()))\n",
    "        console_handler.setFormatter(formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "    # File handler with rotation\n",
    "    if file_output:\n",
    "        # Create log directory\n",
    "        if log_dir:\n",
    "            log_dir = Path(log_dir)\n",
    "            log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Generate log filename\n",
    "        if log_file is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            if include_timestamp_in_filename:\n",
    "                log_filename = f\"{name}_{timestamp}.log\"\n",
    "            else:\n",
    "                log_filename = f\"{name}.log\"\n",
    "            log_file = log_dir / log_filename if log_dir else Path(log_filename)\n",
    "        else:\n",
    "            log_file = Path(log_file)\n",
    "            if log_dir and not log_file.is_absolute():\n",
    "                log_file = Path(log_dir) / log_file\n",
    "\n",
    "        # Create rotating file handler\n",
    "        from logging.handlers import RotatingFileHandler\n",
    "\n",
    "        file_handler = RotatingFileHandler(\n",
    "            log_file, maxBytes=max_bytes, backupCount=backup_count, encoding=\"utf-8\"\n",
    "        )\n",
    "        file_handler.setLevel(getattr(logging, log_level.upper()))\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    # Add some useful methods to the logger\n",
    "    def log_dataset_info(dataset, dataset_name=\"Dataset\"):\n",
    "        \"\"\"Log dataset information\"\"\"\n",
    "        logger.info(f\"{dataset_name} Info:\")\n",
    "        logger.info(f\"  - Size: {len(dataset):,} samples\")\n",
    "        logger.info(f\"  - Columns: {dataset.column_names}\")\n",
    "        if \"labels\" in dataset.column_names:\n",
    "            import numpy as np\n",
    "\n",
    "            labels = np.array(dataset[\"labels\"])\n",
    "            unique, counts = np.unique(labels, return_counts=True)\n",
    "            logger.info(f\"  - Label distribution: {dict(zip(unique, counts))}\")\n",
    "\n",
    "    def log_model_info(model, model_name=\"Model\"):\n",
    "        \"\"\"Log model information\"\"\"\n",
    "        logger.info(f\"{model_name} Info:\")\n",
    "        if hasattr(model, \"config\"):\n",
    "            logger.info(f\"  - Model type: {model.config.model_type}\")\n",
    "            logger.info(f\"  - Hidden size: {model.config.hidden_size}\")\n",
    "            if hasattr(model.config, \"num_labels\"):\n",
    "                logger.info(f\"  - Number of labels: {model.config.num_labels}\")\n",
    "\n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        logger.info(f\"  - Total parameters: {total_params:,}\")\n",
    "        logger.info(f\"  - Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    def log_training_args(training_args):\n",
    "        \"\"\"Log training arguments\"\"\"\n",
    "        logger.info(\"Training Configuration:\")\n",
    "        logger.info(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "        logger.info(f\"  - Batch size: {training_args.per_device_train_batch_size}\")\n",
    "        logger.info(\n",
    "            f\"  - Gradient accumulation: {training_args.gradient_accumulation_steps}\"\n",
    "        )\n",
    "        logger.info(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "        logger.info(f\"  - Weight decay: {training_args.weight_decay}\")\n",
    "        logger.info(f\"  - LR scheduler: {training_args.lr_scheduler_type}\")\n",
    "        logger.info(f\"  - Warmup ratio: {training_args.warmup_ratio}\")\n",
    "\n",
    "    def log_metrics(metrics, stage=\"\"):\n",
    "        \"\"\"Log evaluation metrics\"\"\"\n",
    "        stage_prefix = f\"{stage} \" if stage else \"\"\n",
    "        logger.info(f\"{stage_prefix}Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                logger.info(f\"  - {metric}: {value:.4f}\")\n",
    "            else:\n",
    "                logger.info(f\"  - {metric}: {value}\")\n",
    "\n",
    "    # Attach utility methods to logger\n",
    "    logger.log_dataset_info = log_dataset_info\n",
    "    logger.log_model_info = log_model_info\n",
    "    logger.log_training_args = log_training_args\n",
    "    logger.log_metrics = log_metrics\n",
    "\n",
    "    # Log logger creation\n",
    "    logger.info(f\"Logger '{name}' created successfully\")\n",
    "    logger.info(f\"Log level: {log_level}\")\n",
    "    if file_output:\n",
    "        logger.info(f\"Log file: {log_file}\")\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "# Convenience function for quick setup\n",
    "def setup_project_logging(debug_mode: bool = False) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Quick setup for the Reddit moderation project logging.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    debug_mode : bool\n",
    "        If True, sets log level to DEBUG and enables verbose logging\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    logging.Logger\n",
    "        Configured project logger\n",
    "    \"\"\"\n",
    "    log_level = \"DEBUG\" if debug_mode else \"INFO\"\n",
    "\n",
    "    return create_logger(\n",
    "        name=\"reddit_moderation_pipeline\",\n",
    "        log_level=log_level,\n",
    "        log_dir=\"project_logs\",\n",
    "        include_timestamp_in_filename=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_ram_usage():\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss  # bytes\n",
    "\n",
    "\n",
    "def free_vars(\n",
    "    vars_to_delete: List[Union[str, object]],\n",
    "    namespace: Optional[dict] = None,\n",
    "    try_gpu: bool = True,\n",
    "    logger=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Deletes variables by name or reference, frees RAM and GPU (PyTorch) memory,\n",
    "    logs actions via logger if provided.\n",
    "\n",
    "    Args:\n",
    "      vars_to_delete: list of variable names (str) or object refs\n",
    "      namespace: dict to remove names from (defaults to caller's globals())\n",
    "      try_gpu: clear GPU memory for torch objects\n",
    "      logger: logging object or None (use print)\n",
    "    Returns:\n",
    "      (freed_ram_bytes, freed_gpu_bytes)\n",
    "    \"\"\"\n",
    "    # Setup logger if not provided\n",
    "    if logger is None:\n",
    "\n",
    "        def logger(msg):\n",
    "            print(msg)\n",
    "\n",
    "    else:\n",
    "        logger = logger.info\n",
    "\n",
    "    # Automatic namespace resolution\n",
    "    if namespace is None:\n",
    "        # Get frame of the caller, locals then globals\n",
    "        frame = inspect.currentframe().f_back\n",
    "        namespace = frame.f_globals\n",
    "\n",
    "    before_ram = get_ram_usage()\n",
    "    try:\n",
    "        import torch\n",
    "    except ImportError:\n",
    "        torch = None\n",
    "\n",
    "    freed_gpu_bytes = 0\n",
    "    torch_objs = []\n",
    "    deleted = []\n",
    "\n",
    "    for var in vars_to_delete:\n",
    "        if isinstance(var, str):\n",
    "            obj = namespace.get(var, None)\n",
    "            if obj is not None:\n",
    "                deleted.append(var)\n",
    "                if torch and try_gpu:\n",
    "                    torch_objs.append(obj)\n",
    "                del namespace[var]\n",
    "                logger(f\"Deleted variable '{var}'\")\n",
    "            else:\n",
    "                logger(f\"Variable '{var}' not found in namespace\")\n",
    "        else:\n",
    "            # Try to remove all names referencing the object\n",
    "            names = [n for n, v in namespace.items() if v is var]\n",
    "            for n in names:\n",
    "                del namespace[n]\n",
    "                deleted.append(n)\n",
    "                logger(f\"Deleted variable '{n}' (by reference)\")\n",
    "            if not names:\n",
    "                logger(\n",
    "                    f\"Could not find a variable name for object {var!r}, may not be deleted\"\n",
    "                )\n",
    "            if torch and try_gpu:\n",
    "                torch_objs.append(var)\n",
    "\n",
    "    if torch and try_gpu and torch_objs and torch.cuda.is_available():\n",
    "        before_gpu = torch.cuda.memory_allocated()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        after_gpu = torch.cuda.memory_allocated()\n",
    "        freed_gpu_bytes = after_gpu - before_gpu\n",
    "        logger(f\"GPU memory freed: {freed_gpu_bytes/(1024**2):.2f} MB\")\n",
    "    # Always run gc\n",
    "    gc.collect()\n",
    "    after_ram = get_ram_usage()\n",
    "    freed_ram_bytes = after_ram - before_ram\n",
    "    logger(f\"RAM memory freed: {freed_ram_bytes/(1024**2):.2f} MB\")\n",
    "    clean_mem()\n",
    "    # return freed_ram_bytes, freed_gpu_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:17.215580Z",
     "iopub.status.busy": "2025-08-14T10:58:17.215343Z",
     "iopub.status.idle": "2025-08-14T10:58:17.232497Z",
     "shell.execute_reply": "2025-08-14T10:58:17.231802Z",
     "shell.execute_reply.started": "2025-08-14T10:58:17.215556Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import markdown2\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "def sanitize_comment(comment):\n",
    "    # Convert markdown to HTML, then extract the text (HTML tags removed)\n",
    "    html = markdown2.markdown(comment)\n",
    "    text = BeautifulSoup(html, features=\"html.parser\").get_text()\n",
    "\n",
    "    text = re.sub(r\"\\[([^\\]]+)\\]\\(([^)]+)\\)\", r\"\\1\", comment)\n",
    "    # Then re-run markdown2 and extract text again to clean up\n",
    "    html = markdown2.markdown(text)\n",
    "    text = BeautifulSoup(html, features=\"html.parser\").get_text()\n",
    "\n",
    "    url_pattern = re.compile(r\"((?:http|https)://[^\\s]+|www\\.[^\\s]+)\", re.IGNORECASE)\n",
    "    text = url_pattern.sub(lambda m: m.group(0), text)\n",
    "\n",
    "    # Convert non-unicode characters to unicode (ASCII compatible)\n",
    "    text = unidecode(text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = \" \".join(text.split()).lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:17.233629Z",
     "iopub.status.busy": "2025-08-14T10:58:17.233266Z",
     "iopub.status.idle": "2025-08-14T10:58:17.253744Z",
     "shell.execute_reply": "2025-08-14T10:58:17.252986Z",
     "shell.execute_reply.started": "2025-08-14T10:58:17.233605Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 11:57:18 | rulewise | INFO | Logger 'rulewise' created successfully\n",
      "2025-08-17 11:57:18 | rulewise | INFO | Log level: INFO\n",
      "2025-08-17 11:57:18 | rulewise | INFO | Log file: logs/rulewise_20250817_115718.log\n"
     ]
    }
   ],
   "source": [
    "logger = create_logger(name=\"rulewise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:17.254723Z",
     "iopub.status.busy": "2025-08-14T10:58:17.254491Z",
     "iopub.status.idle": "2025-08-14T10:58:17.297082Z",
     "shell.execute_reply": "2025-08-14T10:58:17.296363Z",
     "shell.execute_reply.started": "2025-08-14T10:58:17.254697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# INPUT_PATH = os.path.join(\"/\", \"kaggle\", \"input\")\n",
    "INPUT_PATH = os.path.join(\"..\", \"data\")\n",
    "train = pd.read_csv(\n",
    "    os.path.join(INPUT_PATH, \"jigsaw-agile-community-rules\", \"train.csv\")\n",
    ")\n",
    "test = pd.read_csv(os.path.join(INPUT_PATH, \"jigsaw-agile-community-rules\", \"test.csv\"))\n",
    "submission = pd.read_csv(\n",
    "    os.path.join(INPUT_PATH, \"jigsaw-agile-community-rules\", \"sample_submission.csv\")\n",
    ")\n",
    "\n",
    "# OUTPUT_PATH = os.path.join(\"/\", \"kaggle\", \"working\")\n",
    "OUTPUT_PATH = os.path.join(\".\", \"rulewise-output\")\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:17.299469Z",
     "iopub.status.busy": "2025-08-14T10:58:17.299163Z",
     "iopub.status.idle": "2025-08-14T10:58:23.630103Z",
     "shell.execute_reply": "2025-08-14T10:58:23.629430Z",
     "shell.execute_reply.started": "2025-08-14T10:58:17.299447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 11:57:18 | rulewise | INFO | Cleaning c = 'body'\n",
      "2025-08-17 11:57:19 | rulewise | INFO | Cleaning c = 'rule'\n",
      "2025-08-17 11:57:19 | rulewise | INFO | Cleaning c = 'subreddit'\n",
      "2025-08-17 11:57:19 | rulewise | INFO | Cleaning c = 'positive_example_1'\n",
      "2025-08-17 11:57:20 | rulewise | INFO | Cleaning c = 'positive_example_2'\n",
      "2025-08-17 11:57:21 | rulewise | INFO | Cleaning c = 'negative_example_1'\n",
      "2025-08-17 11:57:21 | rulewise | INFO | Cleaning c = 'negative_example_2'\n"
     ]
    }
   ],
   "source": [
    "for c in [\n",
    "    \"body\",\n",
    "    \"rule\",\n",
    "    \"subreddit\",\n",
    "    \"positive_example_1\",\n",
    "    \"positive_example_2\",\n",
    "    \"negative_example_1\",\n",
    "    \"negative_example_2\",\n",
    "]:\n",
    "    logger.info(f\"Cleaning {c = }\")\n",
    "    train[c] = train[c].apply(sanitize_comment)\n",
    "    test[c] = test[c].apply(sanitize_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## melt the dataset\n",
    "\n",
    "there will be 3 parts\n",
    "\n",
    "-   actual training\n",
    "-   training examples\n",
    "-   testing examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:23.631091Z",
     "iopub.status.busy": "2025-08-14T10:58:23.630847Z",
     "iopub.status.idle": "2025-08-14T10:58:23.657758Z",
     "shell.execute_reply": "2025-08-14T10:58:23.656948Z",
     "shell.execute_reply.started": "2025-08-14T10:58:23.631070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_main = (\n",
    "    train[[\"row_id\", \"body\", \"rule\", \"rule_violation\"]]\n",
    "    .rename(columns={\"rule_violation\": \"label\"})\n",
    "    .assign(split=\"train\")\n",
    ")\n",
    "train_examples = pd.concat(\n",
    "    [\n",
    "        (\n",
    "            train[[\"row_id\", \"rule\", \"positive_example_1\", \"positive_example_2\"]]\n",
    "            .melt(id_vars=[\"row_id\", \"rule\"], value_name=\"body\")\n",
    "            .assign(label=1, split=\"train\")\n",
    "            .drop(columns=[\"variable\"])\n",
    "        ),\n",
    "        (\n",
    "            train[[\"row_id\", \"rule\", \"negative_example_1\", \"negative_example_2\"]]\n",
    "            .melt(id_vars=[\"row_id\", \"rule\"], value_name=\"body\")\n",
    "            .assign(label=0, split=\"train\")\n",
    "            .drop(columns=[\"variable\"])\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "test_examples = pd.concat(\n",
    "    [\n",
    "        (\n",
    "            test[[\"row_id\", \"rule\", \"positive_example_1\", \"positive_example_2\"]]\n",
    "            .melt(id_vars=[\"row_id\", \"rule\"], value_name=\"body\")\n",
    "            .assign(label=1, split=\"test\")\n",
    "            .drop(columns=[\"variable\"])\n",
    "        ),\n",
    "        (\n",
    "            test[[\"row_id\", \"rule\", \"negative_example_1\", \"negative_example_2\"]]\n",
    "            .melt(id_vars=[\"row_id\", \"rule\"], value_name=\"body\")\n",
    "            .assign(label=0, split=\"test\")\n",
    "            .drop(columns=[\"variable\"])\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:23.658903Z",
     "iopub.status.busy": "2025-08-14T10:58:23.658655Z",
     "iopub.status.idle": "2025-08-14T10:58:23.746618Z",
     "shell.execute_reply": "2025-08-14T10:58:23.745917Z",
     "shell.execute_reply.started": "2025-08-14T10:58:23.658882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fulldata = pd.concat(\n",
    "    [train_main, train_examples[train_main.columns], test_examples[train_main.columns]],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "# since we are using the examples there will be duplicates\n",
    "fulldata = fulldata.drop_duplicates(subset=[\"body\", \"rule\", \"label\"])\n",
    "fulldata.to_csv(os.path.join(OUTPUT_PATH, \"fulldata.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:23.747642Z",
     "iopub.status.busy": "2025-08-14T10:58:23.747410Z",
     "iopub.status.idle": "2025-08-14T10:58:25.283413Z",
     "shell.execute_reply": "2025-08-14T10:58:25.282651Z",
     "shell.execute_reply.started": "2025-08-14T10:58:23.747625Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 11:57:22 | rulewise | INFO | Deleted variable 'train'\n",
      "2025-08-17 11:57:22 | rulewise | INFO | Deleted variable 'train_main'\n",
      "2025-08-17 11:57:22 | rulewise | INFO | Deleted variable 'train_examples'\n",
      "2025-08-17 11:57:22 | rulewise | INFO | Deleted variable 'test_examples'\n",
      "2025-08-17 11:57:24 | rulewise | INFO | GPU memory freed: 0.00 MB\n",
      "2025-08-17 11:57:24 | rulewise | INFO | RAM memory freed: 109.29 MB\n",
      "RAM freed: 0.00 MB (986.05 -> 986.05)\n",
      "GPU allocated freed: 0.00 MB (0.00 -> 0.00)\n",
      "GPU reserved freed: 0.00 MB (0.00 -> 0.00)\n"
     ]
    }
   ],
   "source": [
    "free_vars(\n",
    "    [\"train\", \"train_main\", \"train_examples\", \"test_examples\"],\n",
    "    namespace=globals(),\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset and dataloader\n",
    "\n",
    "when we are creating the dataloader, we should specify the rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:25.284997Z",
     "iopub.status.busy": "2025-08-14T10:58:25.284258Z",
     "iopub.status.idle": "2025-08-14T10:58:25.290940Z",
     "shell.execute_reply": "2025-08-14T10:58:25.290149Z",
     "shell.execute_reply.started": "2025-08-14T10:58:25.284973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CommentDataset(utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        dataset = dataset\n",
    "        self.row_id = dataset[\"row_id\"].tolist()\n",
    "        self.rule = dataset[\"rule\"].tolist()\n",
    "        self.body = dataset[\"body\"].tolist()\n",
    "        self.label = dataset[\"label\"].tolist()\n",
    "\n",
    "        rule_encodings = tokenizer(self.rule, truncation=True, padding=\"max_length\")\n",
    "        comment_encodings = tokenizer(self.body, truncation=True, padding=\"max_length\")\n",
    "\n",
    "        self.rule_input_ids = rule_encodings[\"input_ids\"]\n",
    "        self.rule_attention_mask = rule_encodings[\"attention_mask\"]\n",
    "        self.comment_input_ids = comment_encodings[\"input_ids\"]\n",
    "        self.comment_attention_mask = comment_encodings[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.rule_input_ids[index]),\n",
    "            torch.tensor(self.rule_attention_mask[index]),\n",
    "            torch.tensor(self.comment_input_ids[index]),\n",
    "            torch.tensor(self.comment_attention_mask[index]),\n",
    "            torch.tensor(self.label[index]),\n",
    "            torch.tensor(self.row_id[index]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:25.292028Z",
     "iopub.status.busy": "2025-08-14T10:58:25.291703Z",
     "iopub.status.idle": "2025-08-14T10:58:25.309987Z",
     "shell.execute_reply": "2025-08-14T10:58:25.309244Z",
     "shell.execute_reply.started": "2025-08-14T10:58:25.292005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_datasets(fulldata, test, test_size, rule):\n",
    "    if isinstance(rule, str):\n",
    "        rule = [rule]\n",
    "    subdata = fulldata[fulldata[\"rule\"].isin(rule)]\n",
    "    subtest = test[test[\"rule\"].isin(rule)]\n",
    "\n",
    "    logger.info(f\"{len(subdata)}/{len(fulldata)} rows remain in fulldata.\")\n",
    "    logger.info(f\"{len(subtest)}/{len(test)} rows remain in test.\")\n",
    "    train, val = train_test_split(\n",
    "        subdata,\n",
    "        test_size=test_size,\n",
    "        shuffle=True,\n",
    "        random_state=42,\n",
    "        stratify=subdata[\"label\"],\n",
    "    )\n",
    "    subtest[\"label\"] = 0\n",
    "    columns = [\"row_id\", \"body\", \"rule\", \"label\"]\n",
    "    return train[columns], val[columns], subtest[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:25.311017Z",
     "iopub.status.busy": "2025-08-14T10:58:25.310771Z",
     "iopub.status.idle": "2025-08-14T10:58:25.326678Z",
     "shell.execute_reply": "2025-08-14T10:58:25.325959Z",
     "shell.execute_reply.started": "2025-08-14T10:58:25.310995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataloaders(train, val, test, tokenizer, batch_size=8):\n",
    "    train_dataset = CommentDataset(train, tokenizer)\n",
    "    train_dataloader = utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        # num_workers=4,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    val_dataset = CommentDataset(val, tokenizer)\n",
    "    val_dataloader = utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        # num_workers=4,\n",
    "        batch_size=2 * batch_size,\n",
    "    )\n",
    "    test_dataset = CommentDataset(test, tokenizer)\n",
    "    test_dataloader = utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        # num_workers=4,\n",
    "        batch_size=4 * batch_size,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:25.327821Z",
     "iopub.status.busy": "2025-08-14T10:58:25.327503Z",
     "iopub.status.idle": "2025-08-14T10:58:25.347609Z",
     "shell.execute_reply": "2025-08-14T10:58:25.346885Z",
     "shell.execute_reply.started": "2025-08-14T10:58:25.327800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 11:58:48 | rulewise | INFO | {'classifier': 'FacebookAI/roberta-base'}\n"
     ]
    }
   ],
   "source": [
    "# _MODEL_VERSION_PATH = os.path.join(\n",
    "#     \"transformers\",\n",
    "#     \"default\",\n",
    "#     \"1\",\n",
    "# )\n",
    "# _MODEL_DIR = os.path.join(\"/\", \"kaggle\", \"input\")\n",
    "# MODEL_PATH = {\n",
    "#     \"classifier\": os.path.join(_MODEL_DIR, \"roberta-base\", _MODEL_VERSION_PATH),\n",
    "# }\n",
    "\n",
    "_MODEL_DIR = os.path.join(\"..\", \"model\")\n",
    "MODEL_PATH = {\n",
    "    \"classifier\": \"FacebookAI/roberta-base\",\n",
    "}\n",
    "logger.info(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_labels: int = 2, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # a layer norm to ensure that means and standard deviations are standardised\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        # now take a clf head\n",
    "        # this should bring the compressed thingy\n",
    "        # down to num_labels\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 4, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, rule_output, comment_output):\n",
    "        # sentence similarity thingy from sbert did not work\n",
    "        # back to just the difference\n",
    "        diff = rule_output - comment_output\n",
    "        # apply layer norm\n",
    "        combined = self.layer_norm(diff)\n",
    "        # now do the classification\n",
    "        logits = self.clf(combined)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:25.348564Z",
     "iopub.status.busy": "2025-08-14T10:58:25.348347Z",
     "iopub.status.idle": "2025-08-14T10:58:25.362242Z",
     "shell.execute_reply": "2025-08-14T10:58:25.361578Z",
     "shell.execute_reply.started": "2025-08-14T10:58:25.348549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DifferenceRoberta(RobertaPreTrainedModel):\n",
    "    config_class = RobertaConfig\n",
    "\n",
    "    def __init__(self, config: RobertaConfig):\n",
    "        super().__init__(config)\n",
    "        _basemodel = RobertaForSequenceClassification(config)\n",
    "        self.roberta = _basemodel.roberta\n",
    "        self.classifier = Classifier(\n",
    "            hidden_size=config.hidden_size, num_labels=config.num_labels, dropout=0.5\n",
    "        )\n",
    "        self.hidden_size = config.hidden_size\n",
    "        free_vars([\"_basemodel\"], namespace=locals(), logger=logger)\n",
    "        self.init_weights()\n",
    "        logger.info(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        rule_input_ids=None,\n",
    "        rule_attention_mask=None,\n",
    "        comment_input_ids=None,\n",
    "        comment_attention_mask=None,\n",
    "        return_dict=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "        # take the tensor at [:, 0, :]\n",
    "        # this corresponds to the CLS token\n",
    "        # this token is like a sentence level representation of the input\n",
    "        rule_outputs = self.roberta(\n",
    "            input_ids=rule_input_ids,\n",
    "            attention_mask=rule_attention_mask,\n",
    "            return_dict=True,\n",
    "        ).last_hidden_state[:, 0, :]\n",
    "        comment_outputs = self.roberta(\n",
    "            input_ids=comment_input_ids,\n",
    "            attention_mask=comment_attention_mask,\n",
    "            return_dict=True,\n",
    "        ).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # now pass this through the classifier to get final preds\n",
    "        logits = self.classifier(rule_outputs, comment_outputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:25.364942Z",
     "iopub.status.busy": "2025-08-14T10:58:25.364707Z",
     "iopub.status.idle": "2025-08-14T10:58:25.383621Z",
     "shell.execute_reply": "2025-08-14T10:58:25.382878Z",
     "shell.execute_reply.started": "2025-08-14T10:58:25.364927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BasedRedditMod(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffroberta: DifferenceRoberta,\n",
    "        max_steps: int,\n",
    "        logger: logging.Logger = logger,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.diffroberta = diffroberta\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        self.accuracy = BinaryAccuracy()\n",
    "        self.auroc = BinaryAUROC()\n",
    "        self.previous_auroc = 0\n",
    "\n",
    "        self.mylogger = logger\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        rid, ram, cid, cam, labels, row_ids = batch\n",
    "        logits = self.diffroberta(rid, ram, cid, cam)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, *args, **kwargs):\n",
    "        rid, ram, cid, cam, labels, row_ids = batch\n",
    "        logits = self.diffroberta(rid, ram, cid, cam)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # compute the metrics\n",
    "        probs, preds = self._get_predictions_from_logits(logits, labels, row_ids)\n",
    "        self.accuracy.update(preds=preds, target=labels)\n",
    "        self.auroc.update(preds=probs, target=labels)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\"accuracy\": self.accuracy.compute(), \"auroc\": self.auroc.compute()},\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        accuracy = self.accuracy.compute()\n",
    "        auroc = self.auroc.compute()\n",
    "        change = auroc - self.previous_auroc\n",
    "        self.previous_auroc = auroc\n",
    "\n",
    "        self.accuracy.reset()\n",
    "        self.auroc.reset()\n",
    "\n",
    "        self.mylogger.info(\n",
    "            f\"acc = {accuracy*100.0:.2f} % | auroc = {auroc:.2f} | change = {change:.4f}\"\n",
    "        )\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, *args, **kwargs):\n",
    "        rid, ram, cid, cam, labels, row_ids = batch\n",
    "        logits = self.diffroberta(rid, ram, cid, cam)\n",
    "        probs, _ = self._get_predictions_from_logits(logits, labels, row_ids)\n",
    "        return row_ids, probs\n",
    "\n",
    "    def _get_predictions_from_logits(self, logits, labels, row_ids):\n",
    "        # get the probs and the preds\n",
    "        probs_full = F.softmax(logits, dim=1)\n",
    "        probs = probs_full[:, 1]\n",
    "        preds = torch.argmax(probs_full, dim=1)\n",
    "\n",
    "        assert preds.shape == probs.shape == labels.shape == row_ids.shape\n",
    "\n",
    "        return probs, preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = 2e-5\n",
    "        wd = 0.01\n",
    "        optimiser = optim.AdamW(self.diffroberta.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer=optimiser, T_max=self.max_steps, eta_min=5e-6\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimiser,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:25.384616Z",
     "iopub.status.busy": "2025-08-14T10:58:25.384360Z",
     "iopub.status.idle": "2025-08-14T10:58:25.401934Z",
     "shell.execute_reply": "2025-08-14T10:58:25.401092Z",
     "shell.execute_reply.started": "2025-08-14T10:58:25.384593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# get all the rules\n",
    "all_rules = fulldata[\"rule\"].unique().tolist()\n",
    "id2rule = dict(enumerate(all_rules))\n",
    "rule2id = {v: k for k, v in id2rule.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:25.403636Z",
     "iopub.status.busy": "2025-08-14T10:58:25.402662Z",
     "iopub.status.idle": "2025-08-14T10:58:25.415579Z",
     "shell.execute_reply": "2025-08-14T10:58:25.414901Z",
     "shell.execute_reply.started": "2025-08-14T10:58:25.403614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_predictions(submission_list):\n",
    "    submission = pd.concat(submission_list, ignore_index=True)\n",
    "    submission.to_csv(os.path.join(OUTPUT_PATH, \"submission.csv\"))\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:25.416514Z",
     "iopub.status.busy": "2025-08-14T10:58:25.416278Z",
     "iopub.status.idle": "2025-08-14T10:58:25.432189Z",
     "shell.execute_reply": "2025-08-14T10:58:25.431482Z",
     "shell.execute_reply.started": "2025-08-14T10:58:25.416494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUMULATION_STEPS = 4\n",
    "VALIDATION_PER_N_STEPS = 2 * BATCH_SIZE * GRAD_ACCUMULATION_STEPS\n",
    "# EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:25.433119Z",
     "iopub.status.busy": "2025-08-14T10:58:25.432925Z",
     "iopub.status.idle": "2025-08-14T10:58:25.449601Z",
     "shell.execute_reply": "2025-08-14T10:58:25.448874Z",
     "shell.execute_reply.started": "2025-08-14T10:58:25.433105Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 11:58:54 | rulewise | INFO | {'no advertising: spam, referral links, unsolicited advertising, and promotional content are not allowed.': 0, 'no legal advice: do not offer or request legal advice.': 1}\n"
     ]
    }
   ],
   "source": [
    "# get all the rules\n",
    "all_rules = fulldata[\"rule\"].unique().tolist()\n",
    "id2rule = dict(enumerate(all_rules))\n",
    "rule2id = {v: k for k, v in id2rule.items()}\n",
    "\n",
    "logger.info(rule2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T10:58:25.450727Z",
     "iopub.status.busy": "2025-08-14T10:58:25.450455Z",
     "iopub.status.idle": "2025-08-14T10:58:46.130094Z",
     "shell.execute_reply": "2025-08-14T10:58:46.128778Z",
     "shell.execute_reply.started": "2025-08-14T10:58:25.450707Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8817467d8aeb4c59a5842e2e8d0723e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d501d175183443fbba195ba3c57babb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 11:59:25 | rulewise | INFO | Deleted variable '_basemodel'\n",
      "2025-08-17 11:59:25 | rulewise | INFO | GPU memory freed: 0.00 MB\n",
      "2025-08-17 11:59:25 | rulewise | INFO | RAM memory freed: 0.02 MB\n",
      "RAM freed: 0.00 MB (1165.94 -> 1165.94)\n",
      "GPU allocated freed: 0.00 MB (0.00 -> 0.00)\n",
      "GPU reserved freed: 0.00 MB (0.00 -> 0.00)\n",
      "2025-08-17 11:59:25 | rulewise | INFO | RobertaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DifferenceRoberta were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.clf.0.bias', 'classifier.clf.0.weight', 'classifier.clf.3.bias', 'classifier.clf.3.weight', 'classifier.clf.6.bias', 'classifier.clf.6.weight', 'classifier.layer_norm.bias', 'classifier.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 11:59:25 | rulewise | INFO | ./rulewise-output/rule-0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f154a271a5d474bb2648c31c4836823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814bdc79cd514175a9fb0ccec93ac387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed79d46479342dfad7e208974d65149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34cf79368134e58b02a4fac22cfe28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 11:59:30 | rulewise | INFO | tokenizer.pad_token = '<pad>' | tokenizer.eos_token = '</s>'\n",
      "2025-08-17 11:59:30 | rulewise | INFO | 860/1874 rows remain in fulldata.\n",
      "2025-08-17 11:59:30 | rulewise | INFO | 9/10 rows remain in test.\n",
      "2025-08-17 11:59:30 | rulewise | INFO | train steps = 80 | val steps = 13\n",
      "2025-08-17 11:59:30 | rulewise | INFO | Deleted variable 'train'\n",
      "2025-08-17 11:59:30 | rulewise | INFO | Deleted variable 'val'\n",
      "2025-08-17 11:59:30 | rulewise | INFO | Deleted variable 'subtest'\n",
      "2025-08-17 11:59:30 | rulewise | INFO | GPU memory freed: 0.00 MB\n",
      "2025-08-17 11:59:30 | rulewise | INFO | RAM memory freed: 0.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | diffroberta | DifferenceRoberta | 124 M  | train\n",
      "1 | loss_fn     | CrossEntropyLoss  | 0      | train\n",
      "2 | accuracy    | BinaryAccuracy    | 0      | train\n",
      "3 | auroc       | BinaryAUROC       | 0      | train\n",
      "----------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "497.705   Total estimated model params size (MB)\n",
      "239       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM freed: 0.00 MB (1264.33 -> 1264.33)\n",
      "GPU allocated freed: 0.00 MB (0.00 -> 0.00)\n",
      "GPU reserved freed: 0.00 MB (0.00 -> 0.00)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f7e0a532754d959db514ce3abf099a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 11:59:31 | rulewise | INFO | acc = 62.50 % | auroc = 0.71 | change = 0.7098\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c8d9019a62415099ed200002ff7101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b95d449d55463986e53bfafcf80f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 11:59:53 | rulewise | INFO | acc = 55.77 % | auroc = 0.77 | change = 0.0569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric auroc improved. New best score: 0.710\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cc256c311a4b7fa5cc6be604a8d460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:00:24 | rulewise | INFO | acc = 67.31 % | auroc = 0.87 | change = 0.1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric auroc improved by 0.133 >= min_delta = 0.0001. New best score: 0.843\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95d19266626400ab5b391627a513212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:00:55 | rulewise | INFO | acc = 79.81 % | auroc = 0.92 | change = 0.0477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric auroc improved by 0.069 >= min_delta = 0.0001. New best score: 0.911\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc51f9fe28134df1ad910cc6e1f64015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:01:26 | rulewise | INFO | acc = 83.17 % | auroc = 0.92 | change = 0.0010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c416909cef44be69ecbd0593d85c460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:01:55 | rulewise | INFO | acc = 82.21 % | auroc = 0.91 | change = -0.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric auroc improved by 0.012 >= min_delta = 0.0001. New best score: 0.924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f7d8844e4d4ade9328518a61054a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:02:26 | rulewise | INFO | acc = 78.37 % | auroc = 0.87 | change = -0.0404\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33af3017d6304c5ca9fc8a59f3b315b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:02:55 | rulewise | INFO | acc = 80.77 % | auroc = 0.89 | change = 0.0193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric auroc did not improve in the last 2 records. Best score: 0.924. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:02:58 | rulewise | INFO | Deleted variable 'model'\n",
      "2025-08-17 12:02:58 | rulewise | INFO | Deleted variable 'tokenizer'\n",
      "2025-08-17 12:02:58 | rulewise | INFO | Deleted variable 'train_dataloader'\n",
      "2025-08-17 12:02:58 | rulewise | INFO | Deleted variable 'val_dataloader'\n",
      "2025-08-17 12:02:58 | rulewise | INFO | GPU memory freed: 0.00 MB\n",
      "2025-08-17 12:02:58 | rulewise | INFO | RAM memory freed: 0.00 MB\n",
      "RAM freed: 0.00 MB (3537.06 -> 3537.06)\n",
      "GPU allocated freed: 0.00 MB (0.00 -> 0.00)\n",
      "GPU reserved freed: 1596.00 MB (1600.00 -> 4.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568e0fb9f5934a73a5e9b4ce8cb86754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:03:00 | rulewise | INFO | Deleted variable 'outs'\n",
      "2025-08-17 12:03:00 | rulewise | INFO | GPU memory freed: 0.00 MB\n",
      "2025-08-17 12:03:00 | rulewise | INFO | RAM memory freed: 0.00 MB\n",
      "RAM freed: 0.00 MB (3752.79 -> 3752.79)\n",
      "GPU allocated freed: 0.00 MB (0.00 -> 0.00)\n",
      "GPU reserved freed: 0.00 MB (4.00 -> 4.00)\n",
      "2025-08-17 12:03:01 | rulewise | INFO | Deleted variable 'subtest_dataloader'\n",
      "2025-08-17 12:03:01 | rulewise | INFO | Deleted variable 'row_ids'\n",
      "2025-08-17 12:03:01 | rulewise | INFO | Deleted variable 'probs'\n",
      "2025-08-17 12:03:01 | rulewise | INFO | Deleted variable 'trainer'\n",
      "2025-08-17 12:03:01 | rulewise | INFO | GPU memory freed: 0.00 MB\n",
      "2025-08-17 12:03:01 | rulewise | INFO | RAM memory freed: 0.00 MB\n",
      "RAM freed: 0.00 MB (3752.79 -> 3752.79)\n",
      "GPU allocated freed: 0.00 MB (0.00 -> 0.00)\n",
      "GPU reserved freed: 0.00 MB (4.00 -> 4.00)\n",
      "2025-08-17 12:03:01 | rulewise | INFO | ./rulewise-output/rule-1\n",
      "2025-08-17 12:03:02 | rulewise | INFO | tokenizer.pad_token = '<pad>' | tokenizer.eos_token = '</s>'\n",
      "2025-08-17 12:03:02 | rulewise | INFO | 1014/1874 rows remain in fulldata.\n",
      "2025-08-17 12:03:02 | rulewise | INFO | 1/10 rows remain in test.\n",
      "2025-08-17 12:03:02 | rulewise | INFO | train steps = 95 | val steps = 15\n",
      "2025-08-17 12:03:02 | rulewise | INFO | Deleted variable 'train'\n",
      "2025-08-17 12:03:02 | rulewise | INFO | Deleted variable 'val'\n",
      "2025-08-17 12:03:02 | rulewise | INFO | Deleted variable 'subtest'\n",
      "2025-08-17 12:03:02 | rulewise | INFO | GPU memory freed: 0.00 MB\n",
      "2025-08-17 12:03:02 | rulewise | INFO | RAM memory freed: 0.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | diffroberta | DifferenceRoberta | 124 M  | train\n",
      "1 | loss_fn     | CrossEntropyLoss  | 0      | train\n",
      "2 | accuracy    | BinaryAccuracy    | 0      | train\n",
      "3 | auroc       | BinaryAUROC       | 0      | train\n",
      "----------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "497.705   Total estimated model params size (MB)\n",
      "239       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM freed: 0.00 MB (3766.40 -> 3766.40)\n",
      "GPU allocated freed: 0.00 MB (0.00 -> 0.00)\n",
      "GPU reserved freed: 0.00 MB (4.00 -> 4.00)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e79087a4c844d5b0f0ba6411be376c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:03:03 | rulewise | INFO | acc = 56.25 % | auroc = 0.61 | change = 0.6071\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fc779c440142a08415e2a537b232fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f8365443cd44f5bf2d7942248faf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:03:25 | rulewise | INFO | acc = 57.92 % | auroc = 0.65 | change = 0.0405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric auroc improved. New best score: 0.648\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b358a6fda3a4e37afb6f20ad5bc4482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:04:02 | rulewise | INFO | acc = 72.08 % | auroc = 0.74 | change = 0.0962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric auroc improved by 0.112 >= min_delta = 0.0001. New best score: 0.760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101e2690c1ec47d3975ebfbfc5220dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:04:38 | rulewise | INFO | acc = 72.50 % | auroc = 0.80 | change = 0.0550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric auroc improved by 0.063 >= min_delta = 0.0001. New best score: 0.823\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0deac13fea6747f9a989affbbbad16dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:05:13 | rulewise | INFO | acc = 71.25 % | auroc = 0.76 | change = -0.0372\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce8807c5ed94dc089a5ea037f7d20cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:05:47 | rulewise | INFO | acc = 70.83 % | auroc = 0.78 | change = 0.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric auroc did not improve in the last 2 records. Best score: 0.823. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:05:49 | rulewise | INFO | Deleted variable 'model'\n",
      "2025-08-17 12:05:49 | rulewise | INFO | Deleted variable 'tokenizer'\n",
      "2025-08-17 12:05:49 | rulewise | INFO | Deleted variable 'train_dataloader'\n",
      "2025-08-17 12:05:49 | rulewise | INFO | Deleted variable 'val_dataloader'\n",
      "2025-08-17 12:05:49 | rulewise | INFO | GPU memory freed: 0.00 MB\n",
      "2025-08-17 12:05:50 | rulewise | INFO | RAM memory freed: -294.52 MB\n",
      "RAM freed: 0.00 MB (4493.21 -> 4493.21)\n",
      "GPU allocated freed: 0.00 MB (0.00 -> 0.00)\n",
      "GPU reserved freed: 1594.00 MB (1600.00 -> 6.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4152dec4ad24c27b3073c4d4e61d9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-17 12:05:52 | rulewise | INFO | Deleted variable 'outs'\n",
      "2025-08-17 12:05:52 | rulewise | INFO | GPU memory freed: 0.00 MB\n",
      "2025-08-17 12:05:52 | rulewise | INFO | RAM memory freed: 0.00 MB\n",
      "RAM freed: 0.00 MB (4407.41 -> 4407.41)\n",
      "GPU allocated freed: 0.00 MB (0.00 -> 0.00)\n",
      "GPU reserved freed: 0.00 MB (4.00 -> 4.00)\n",
      "2025-08-17 12:05:52 | rulewise | INFO | Deleted variable 'subtest_dataloader'\n",
      "2025-08-17 12:05:52 | rulewise | INFO | Deleted variable 'row_ids'\n",
      "2025-08-17 12:05:52 | rulewise | INFO | Deleted variable 'probs'\n",
      "2025-08-17 12:05:52 | rulewise | INFO | Deleted variable 'trainer'\n",
      "2025-08-17 12:05:52 | rulewise | INFO | GPU memory freed: 0.00 MB\n",
      "2025-08-17 12:05:53 | rulewise | INFO | RAM memory freed: 0.00 MB\n",
      "RAM freed: 0.00 MB (4407.41 -> 4407.41)\n",
      "GPU allocated freed: 0.00 MB (0.00 -> 0.00)\n",
      "GPU reserved freed: 0.00 MB (4.00 -> 4.00)\n"
     ]
    }
   ],
   "source": [
    "all_submissions = []\n",
    "# ------------------------\n",
    "# initialise the tokenizer\n",
    "# ------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH[\"classifier\"])\n",
    "logger.info(f\"{tokenizer.pad_token = } | {tokenizer.eos_token = }\")\n",
    "# --------------------\n",
    "# initialise the model\n",
    "# --------------------\n",
    "diffroberta = DifferenceRoberta.from_pretrained(MODEL_PATH[\"classifier\"], num_labels=2)\n",
    "diffroberta.train()\n",
    "model = BasedRedditMod(\n",
    "    diffroberta=diffroberta, max_steps=4 * VALIDATION_PER_N_STEPS, logger=logger\n",
    ")\n",
    "for rule in all_rules:\n",
    "\n",
    "    # -------------------------------\n",
    "    # get the rule\n",
    "    # make the output paths and stuff\n",
    "    # -------------------------------\n",
    "    rule_output_path = os.path.join(OUTPUT_PATH, f\"rule-{rule2id[rule]}\")\n",
    "    os.makedirs(rule_output_path, exist_ok=True)\n",
    "    logger.info(rule_output_path)\n",
    "\n",
    "    # --------------------------\n",
    "    # prepare the datasets\n",
    "    # as well as the dataloaders\n",
    "    # --------------------------\n",
    "    train, val, subtest = prepare_datasets(fulldata.copy(), test.copy(), 0.25, rule)\n",
    "    train_dataloader, val_dataloader, subtest_dataloader = prepare_dataloaders(\n",
    "        train, val, subtest, tokenizer, batch_size=BATCH_SIZE\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"train steps = {len(train_dataloader)} | val steps = {len(val_dataloader)}\"\n",
    "    )\n",
    "\n",
    "    free_vars([\"train\", \"val\", \"subtest\"], namespace=globals(), logger=logger)\n",
    "\n",
    "    # ---------\n",
    "    # callbacks\n",
    "    # ---------\n",
    "    # ideally i want 2 checkpoints to be saved\n",
    "    # the best one\n",
    "    # the latest one\n",
    "    # i want the checkpoints to be saved immediately after validation check is performed\n",
    "    checkpoint_callback = lcb.ModelCheckpoint(\n",
    "        monitor=\"auroc\",\n",
    "        dirpath=rule_output_path,\n",
    "        mode=\"max\",\n",
    "        save_top_k=1,\n",
    "        save_last=True,\n",
    "        save_on_train_epoch_end=False,\n",
    "    )\n",
    "    # have two early stopping callbacks\n",
    "    # primary - stop if auroc does not improve much\n",
    "    # secondary - stop if val loss does not improve much\n",
    "    early_stopping_callback_auroc = lcb.EarlyStopping(\n",
    "        monitor=\"auroc\", min_delta=1e-4, patience=2, mode=\"max\", verbose=True\n",
    "    )\n",
    "    early_stopping_callback_loss = lcb.EarlyStopping(\n",
    "        monitor=\"val_loss\", min_delta=1e-5, patience=4, mode=\"min\", verbose=True\n",
    "    )\n",
    "\n",
    "    # ---------------------\n",
    "    # customise the trainer\n",
    "    # ---------------------\n",
    "    trainer = L.Trainer(\n",
    "        # limit_train_batches=2 * VALIDATION_PER_N_STEPS,  # this is only for rapid iteration\n",
    "        max_steps=8 * VALIDATION_PER_N_STEPS,\n",
    "        # max_epochs=1,\n",
    "        accelerator=\"cuda\",\n",
    "        # devices=2,\n",
    "        # train in mixed bf16 precision\n",
    "        precision=\"bf16-mixed\",\n",
    "        # each training batch size is 8\n",
    "        # accumulate gradients over 4 batches... so eff. batch size is 32\n",
    "        accumulate_grad_batches=GRAD_ACCUMULATION_STEPS,\n",
    "        # clip gradients' global norm to <=0.5 using gradient_clip_algorithm='norm' by default\n",
    "        gradient_clip_val=0.5,\n",
    "        # perform eval every 32 steps\n",
    "        val_check_interval=VALIDATION_PER_N_STEPS,\n",
    "        # model checkpointing\n",
    "        default_root_dir=rule_output_path,\n",
    "        # callbacks\n",
    "        callbacks=[\n",
    "            checkpoint_callback,\n",
    "            early_stopping_callback_auroc,\n",
    "            # early_stopping_callback_loss,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # -------------\n",
    "    # fit the model\n",
    "    # -------------\n",
    "    model.train()\n",
    "    trainer.fit(\n",
    "        model=model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader\n",
    "    )\n",
    "    free_vars(\n",
    "        [\"model\", \"train_dataloader\", \"val_dataloader\"],\n",
    "        namespace=globals(),\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # load the best model and use it for inference\n",
    "    # --------------------------------------------\n",
    "    best_ckpt = checkpoint_callback.best_model_path\n",
    "    model = BasedRedditMod.load_from_checkpoint(\n",
    "        best_ckpt,\n",
    "        diffroberta=diffroberta,\n",
    "        max_steps=8 * VALIDATION_PER_N_STEPS,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    # -----------------\n",
    "    # write predictions\n",
    "    # -----------------\n",
    "    model.eval()\n",
    "    # this will return a list of length = len(dataloader)\n",
    "    # each element will be a tuple of length 3\n",
    "    # tuple = (row_ids, probs)\n",
    "    outs = trainer.predict(model=model, dataloaders=subtest_dataloader)\n",
    "    row_ids = torch.cat([o[0] for o in outs], dim=0).detach().cpu().numpy()\n",
    "    probs = torch.cat([o[1] for o in outs], dim=0).detach().cpu().numpy()\n",
    "\n",
    "    free_vars([\"outs\"], namespace=globals(), logger=logger)\n",
    "    _submission_for_this_rule = pd.DataFrame(\n",
    "        {\"row_id\": row_ids, \"rule_violation\": probs}\n",
    "    )\n",
    "    all_submissions.append(_submission_for_this_rule)\n",
    "\n",
    "    free_vars(\n",
    "        [\"subtest_dataloader\", \"row_ids\", \"probs\", \"trainer\"],\n",
    "        namespace=globals(),\n",
    "        logger=logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-14T10:58:46.130547Z",
     "iopub.status.idle": "2025-08-14T10:58:46.130842Z",
     "shell.execute_reply": "2025-08-14T10:58:46.130712Z",
     "shell.execute_reply.started": "2025-08-14T10:58:46.130695Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>rule_violation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2029</td>\n",
       "      <td>0.206254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2031</td>\n",
       "      <td>0.680879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2032</td>\n",
       "      <td>0.675336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2033</td>\n",
       "      <td>0.683842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2034</td>\n",
       "      <td>0.203708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2035</td>\n",
       "      <td>0.685949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2036</td>\n",
       "      <td>0.208179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2037</td>\n",
       "      <td>0.205615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2038</td>\n",
       "      <td>0.677047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2030</td>\n",
       "      <td>0.490602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  rule_violation\n",
       "0    2029        0.206254\n",
       "1    2031        0.680879\n",
       "2    2032        0.675336\n",
       "3    2033        0.683842\n",
       "4    2034        0.203708\n",
       "5    2035        0.685949\n",
       "6    2036        0.208179\n",
       "7    2037        0.205615\n",
       "8    2038        0.677047\n",
       "9    2030        0.490602"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_predictions(all_submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "modelId": 418776,
     "modelInstanceId": 400554,
     "sourceId": 504282,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
